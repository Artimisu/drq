actor_entropy,actor_loss,actor_target_entropy,alpha_loss,alpha_value,batch_reward,critic_loss,duration,episode,episode_reward,step
0.8705986061148228,-0.11498880705663136,-2.0,0.2864378405114015,0.09981160906572191,0.04159375,0.6907921732664108,55.05341935157776,9.0,9.0,9000
1.350676942256189,-0.35885248910034856,-2.0,0.3323136395023715,0.0991779183668475,0.036984375,0.21164558136463166,53.22115087509155,10.0,0.0,10000
1.2644837735191223,-0.44139277272754246,-2.0,0.32171261830935405,0.09854643720956076,0.043734375,0.12517109233140947,53.3595871925354,11.0,14.0,11000
1.1585305352364816,-0.5261741194032854,-2.0,0.309361040111511,0.0979440655679472,0.042953125,0.14210227623581886,54.3205041885376,12.0,3.0,12000
